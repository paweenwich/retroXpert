{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [13:35:04] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('./util')\n",
    "from rdchiral.template_extractor import extract_from_reaction\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset',\n",
    "#                     type=str,\n",
    "#                     default='USPTO50K',\n",
    "#                     help='dataset: USPTO50K')\n",
    "# parser.add_argument('--extract_pattern',\n",
    "#                     action='store_true',\n",
    "#                     default=False,\n",
    "#                     help='if extract pattern from training data')\n",
    "# parser.add_argument('--min_freq',\n",
    "#                     type=int,\n",
    "#                     default=2,\n",
    "#                     help='minimum frequency for patterns to be kept')\n",
    "\n",
    "#args = parser.parse_args()\n",
    "#print('extract semi templates for dataset {}...'.format(args.dataset))\n",
    "#assert args.dataset in ['USPTO50K', 'USPTO-full']\n",
    "\n",
    "# patterns_filtered = []\n",
    "# pattern_file = os.path.join('./data', args.dataset, 'product_patterns.txt')\n",
    "# if not args.extract_pattern and os.path.exists(pattern_file):\n",
    "#     print('load semi template patterns from file:', pattern_file)\n",
    "#     with open(pattern_file) as f:\n",
    "#         patterns = f.readlines()\n",
    "#     for p in patterns:\n",
    "#         pa, cnt = p.strip().split(': ')\n",
    "#         if int(cnt) >= args.min_freq:\n",
    "#             patterns_filtered.append(pa)\n",
    "#     print('total number of semi template patterns:', len(patterns_filtered))\n",
    "patterns_filtered = []\n",
    "pattern_file = \"./data/USPTO50K/product_patterns.txt\"\n",
    "args_dataset = \"USPTO50K\"\n",
    "\n",
    "def get_tpl(task):\n",
    "    idx, react, prod = task\n",
    "    reaction = {'_id': idx, 'reactants': react, 'products': prod}\n",
    "    template = extract_from_reaction(reaction, super_general=True)\n",
    "    return idx, template\n",
    "\n",
    "\n",
    "def cano_smarts(smarts):\n",
    "    tmp = Chem.MolFromSmarts(smarts)\n",
    "    if tmp is None:\n",
    "        return None, smarts\n",
    "    [a.ClearProp('molAtomMapNumber') for a in tmp.GetAtoms()]\n",
    "    cano = Chem.MolToSmarts(tmp)\n",
    "    if '[[se]]' in cano:  # strange parse error\n",
    "        cano = smarts\n",
    "    return cano\n",
    "\n",
    "\n",
    "def find_all_patterns(task):\n",
    "    k, product = task\n",
    "    product_mol = Chem.MolFromSmiles(product)\n",
    "    [a.SetAtomMapNum(0) for a in product_mol.GetAtoms()]\n",
    "    matches_all = {}\n",
    "    for idx, pattern in enumerate(patterns_filtered):\n",
    "        pattern_mol = Chem.MolFromSmarts(pattern)\n",
    "        if pattern_mol is None:\n",
    "            print('error: pattern_mol is None')\n",
    "        try:\n",
    "            matches = product_mol.GetSubstructMatches(pattern_mol,\n",
    "                                                      useChirality=False)\n",
    "        except:\n",
    "            continue\n",
    "        else:\n",
    "            if len(matches) > 0 and len(matches[0]) > 0:\n",
    "                matches_all[idx] = matches\n",
    "    if len(matches_all) == 0:\n",
    "        print(product)\n",
    "    num_atoms = product_mol.GetNumAtoms()\n",
    "    pattern_feature = np.zeros((len(patterns_filtered), num_atoms))\n",
    "    for idx, matches in matches_all.items():\n",
    "        if len(matches) > 1 and isinstance(matches[0], tuple):\n",
    "            for match in matches:\n",
    "                np.put(pattern_feature[idx], match, 1)\n",
    "        else:\n",
    "            np.put(pattern_feature[idx], matches, 1)\n",
    "    pattern_feature = pattern_feature.transpose().astype('bool_')\n",
    "    return k, pattern_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_set= train 40008 40008\n"
     ]
    }
   ],
   "source": [
    "for data_set in ['train']:   #['train', 'valid', 'test']:\n",
    "    data_dir = os.path.join('./data', args_dataset, data_set)\n",
    "    data_files = [f for f in os.listdir(data_dir) if f.endswith('.pkl')]\n",
    "    data_files.sort()\n",
    "    products = []\n",
    "    reactants = []\n",
    "    for data_file in data_files:\n",
    "        with open(os.path.join(data_dir, data_file), 'rb') as f:\n",
    "            reaction_data = pickle.load(f)\n",
    "        products.append(\n",
    "            Chem.MolToSmiles(reaction_data['product_mol'], canonical=False))\n",
    "        reactants.append(\n",
    "            Chem.MolToSmiles(reaction_data['reactant_mol'], canonical=False))\n",
    "    \n",
    "    print(\"data_set=\",data_set,len(products),len(reactants))\n",
    "\n",
    "        # idx, template = result\n",
    "        # if 'reaction_smarts' not in template:\n",
    "        #     continue\n",
    "        # product_pattern = cano_smarts(template['products'])\n",
    "        # if product_pattern not in patterns:\n",
    "        #     patterns[product_pattern] = 1\n",
    "        # else:\n",
    "        #     patterns[product_pattern] += 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training rxns: 40008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40008 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if data_set == 'train' and len(patterns_filtered) == 0:\n",
    "        patterns = {}\n",
    "        rxns = []\n",
    "        for idx, r in enumerate(reactants):\n",
    "                rxns.append((idx, r, products[idx]))\n",
    "        print('total training rxns:', len(rxns))\n",
    "\n",
    "        pool = multiprocessing.Pool(1)\n",
    "        for result in tqdm(pool.imap_unordered(get_tpl, rxns),total=len(rxns)):\n",
    "                print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def task1(x):\n",
    "    return x+10\n",
    "pool = multiprocessing.Pool(1)\n",
    "for x in tqdm(pool.imap_unordered(task1,[1,2,3,4]),total=4):\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('retroXpert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e96683d29ed685688ca04abe69e8a61a5d587eed448109638a4988517c04595"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
