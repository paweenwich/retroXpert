{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dsgiitr.com/blogs/deepwalk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 1, 3, 2, 0, 3, 0, 3, 1, 2, 3, 0, 3, 2, 3, 2, 3, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_list = [[1,2,3], [0,2,3], [0, 1, 3], [0, 1, 2], [5, 6], [4,6], [4, 5], [1, 3]]\n",
    "size_vertex = len(adj_list)  # number of vertices\n",
    "w=3            # window size\n",
    "d=2            # embedding size\n",
    "y=200          # walks per vertex\n",
    "t=6            # walk length \n",
    "lr=0.025       # learning rate\n",
    "\n",
    "v=[0,1,2,3,4,5,6,7] # labels of available vertices\n",
    "\n",
    "def RandomWalk(node,t):\n",
    "    walk = [node]        # Walk starts from this node\n",
    "    \n",
    "    for i in range(t-1):\n",
    "        node = adj_list[node][random.randint(0,len(adj_list[node])-1)]\n",
    "        walk.append(node)\n",
    "\n",
    "    return walk\n",
    "\n",
    "\n",
    "RandomWalk(0,20)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.1925, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.1925, 0.3241, 0.4231, 0.1002, 0.7642, 0.6359, 1.0157, 0.8128],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(0.0981, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.2318, 0.3209, 0.4222, 0.0981, 0.7635, 0.6337, 1.0193, 0.8104],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(1.2100, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.2100, 0.3139, 0.4139, 0.1301, 0.7497, 0.6219, 1.0013, 0.7956],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.5082, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5082, 0.0754, 0.1462, 0.0417, 0.2557, 0.1896, 0.4152, 0.2410],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.0418, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5268, 0.0750, 0.1487, 0.0418, 0.2601, 0.1918, 0.4258, 0.2439],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.5132, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5132, 0.0709, 0.1438, 0.0457, 0.2515, 0.1844, 0.4149, 0.2345],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.1463, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5321, 0.0705, 0.1463, 0.0460, 0.2560, 0.1867, 0.4256, 0.2375],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(1.2677, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.2677, 0.3051, 0.4209, 0.1365, 0.7423, 0.6136, 0.9970, 0.7866],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(0.1348, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.3085, 0.3022, 0.4205, 0.1348, 0.7422, 0.6120, 1.0014, 0.7849],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(1.2858, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.2858, 0.2956, 0.4124, 0.1666, 0.7291, 0.6008, 0.9843, 0.7709],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(0.4121, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.3269, 0.2928, 0.4121, 0.1651, 0.7293, 0.5995, 0.9890, 0.7695],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(0.1614, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.3099, 0.2881, 0.4420, 0.1614, 0.7200, 0.5916, 0.9766, 0.7598],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "2 tensor(0.3940, grad_fn=<SelectBackward>) tensor([0., 0., 1., 0., 0., 0., 0., 0.]) tensor([0.3940, 0.3648, 0.2537, 0.1234, 0.4820, 0.5185, 0.2529, 0.6735],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "2 tensor(0.1231, grad_fn=<SelectBackward>) tensor([0., 0., 1., 0., 0., 0., 0., 0.]) tensor([0.4284, 0.3623, 0.2555, 0.1231, 0.4848, 0.5186, 0.2635, 0.6739],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "2 tensor(0.4152, grad_fn=<SelectBackward>) tensor([0., 0., 1., 0., 0., 0., 0., 0.]) tensor([0.4152, 0.3566, 0.2494, 0.1397, 0.4754, 0.5099, 0.2537, 0.6630],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "2 tensor(0.2513, grad_fn=<SelectBackward>) tensor([0., 0., 1., 0., 0., 0., 0., 0.]) tensor([0.4494, 0.3544, 0.2513, 0.1394, 0.4786, 0.5104, 0.2645, 0.6639],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "2 tensor(0.1365, grad_fn=<SelectBackward>) tensor([0., 0., 1., 0., 0., 0., 0., 0.]) tensor([0.4415, 0.3502, 0.2672, 0.1365, 0.4724, 0.5044, 0.2586, 0.6566],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "2 tensor(0.4284, grad_fn=<SelectBackward>) tensor([0., 0., 1., 0., 0., 0., 0., 0.]) tensor([0.4284, 0.3449, 0.2611, 0.1525, 0.4634, 0.4961, 0.2490, 0.6462],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.0723, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5437, 0.0601, 0.1581, 0.0723, 0.2397, 0.1721, 0.4060, 0.2200],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.5313, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5313, 0.0569, 0.1535, 0.0755, 0.2328, 0.1662, 0.3970, 0.2126],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.1570, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5520, 0.0570, 0.1570, 0.0768, 0.2381, 0.1693, 0.4083, 0.2167],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.0751, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5447, 0.0550, 0.1602, 0.0751, 0.2340, 0.1658, 0.4029, 0.2123],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "3 tensor(0.5324, grad_fn=<SelectBackward>) tensor([0., 0., 0., 1., 0., 0., 0., 0.]) tensor([0.5324, 0.0519, 0.1556, 0.0782, 0.2272, 0.1600, 0.3941, 0.2050],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(1.3368, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.3368, 0.2638, 0.4453, 0.2345, 0.6868, 0.5603, 0.9405, 0.7234],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(0.4458, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.3784, 0.2615, 0.4458, 0.2337, 0.6879, 0.5599, 0.9459, 0.7232],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(0.2296, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.3614, 0.2572, 0.4751, 0.2296, 0.6795, 0.5528, 0.9345, 0.7144],\n",
      "       grad_fn=<SqueezeBackward3>)\n",
      "0 tensor(1.3397, grad_fn=<SelectBackward>) tensor([1., 0., 0., 0., 0., 0., 0., 0.]) tensor([1.3397, 0.2519, 0.4670, 0.2594, 0.6685, 0.5436, 0.9199, 0.7030],\n",
      "       grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.phi  = nn.Parameter(torch.rand((size_vertex, d), requires_grad=True))    \n",
    "        self.phi2 = nn.Parameter(torch.rand((d, size_vertex), requires_grad=True))\n",
    "        \n",
    "        \n",
    "    def forward(self, one_hot):\n",
    "        hidden = torch.matmul(one_hot, self.phi)\n",
    "        out    = torch.matmul(hidden, self.phi2)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "\n",
    "#http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "#https://dsgiitr.com/blogs/gcn/\n",
    "def skip_gram(wvi,  w):\n",
    "    for j in range(len(wvi)):\n",
    "        for k in range(max(0,j-w) , min(j+w, len(wvi))):\n",
    "            #print(j,k)\n",
    "            #generate one hot vector\n",
    "            one_hot          = torch.zeros(size_vertex)\n",
    "            one_hot[wvi[j]]  = 1\n",
    "            out              = model(one_hot)\n",
    "            print(wvi[j],out[wvi[k]],one_hot,out)\n",
    "            loss             = torch.log(torch.sum(torch.exp(out))) - out[wvi[k]]\n",
    "            loss.backward()\n",
    "            \n",
    "            for param in model.parameters():\n",
    "                param.data.sub_(lr*param.grad)\n",
    "                param.grad.data.zero_()\n",
    "        \n",
    "\n",
    "# for i in range(y):\n",
    "#     random.shuffle(v)\n",
    "#     for vi in v:\n",
    "#         wvi=RandomWalk(vi,t)\n",
    "#         skip_gram(wvi, w)\n",
    "\n",
    "\n",
    "skip_gram(RandomWalk(0,t), w)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2360,  1.3795],\n",
      "        [-0.5267,  1.2029],\n",
      "        [-0.2580,  1.2695],\n",
      "        [-0.5400,  1.0176],\n",
      "        [ 1.3432, -0.7419],\n",
      "        [ 0.9130, -1.1489],\n",
      "        [ 0.7026, -1.2309],\n",
      "        [-1.3383,  0.1891]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.phi)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('retroXpert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e96683d29ed685688ca04abe69e8a61a5d587eed448109638a4988517c04595"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
